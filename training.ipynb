{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),  # Converts image to (C, H, W)\n",
    "])\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448), interpolation=InterpolationMode.NEAREST),  # Nearest-neighbor\n",
    "    transforms.PILToTensor(),  # Converts to tensor without normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from natsort import natsorted  # For natural sorting (e.g., \"image_10\" comes after \"image_9\")\n",
    "\n",
    "class TripletImageMaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        # List and sort image/mask filenames\n",
    "        self.image_files = natsorted([f for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "        self.mask_files = natsorted([f for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # Ensure the number of images and masks match\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Mismatch between images and masks!\"\n",
    "        \n",
    "        # Exclude first and last images to form triplets (t-1, t, t+1)\n",
    "        self.valid_indices = list(range(1, len(self.image_files)-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the index for the middle image (t)\n",
    "        t = self.valid_indices[idx]\n",
    "        \n",
    "        # Load triplet images: t-1, t, t+1\n",
    "        img_t_minus1 = Image.open(os.path.join(self.image_dir, self.image_files[t-1])).convert(\"RGB\")\n",
    "        img_t = Image.open(os.path.join(self.image_dir, self.image_files[t])).convert(\"RGB\")\n",
    "        img_t_plus1 = Image.open(os.path.join(self.image_dir, self.image_files[t+1])).convert(\"RGB\")\n",
    "        \n",
    "        # Load mask for the middle image (t)\n",
    "        mask_t = Image.open(os.path.join(self.mask_dir, self.mask_files[t])).convert(\"L\")  # Grayscale\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img_t_minus1 = self.transform(img_t_minus1)\n",
    "            img_t = self.transform(img_t)\n",
    "            img_t_plus1 = self.transform(img_t_plus1)\n",
    "            mask_t = self.mask_transform(mask_t)\n",
    "        \n",
    "        # Stack images along the batch dimension (B=3 for triplet)\n",
    "        input_stack = torch.stack([img_t_minus1, img_t, img_t_plus1], dim=0)  # Shape: (3, C, H, W)\n",
    "\n",
    "        print(\"Unique values in mask_t:\", torch.unique(mask_t))\n",
    "\n",
    "        return input_stack, mask_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Split dataset into training and validation\\ntrain_size = int(0.8 * len(dataset))\\nval_size = len(dataset) - train_size\\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\\n\\n# Create DataLoaders\\nbatch_size = 8\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = TripletImageMaskDataset(\n",
    "    image_dir=\"Mask_Data/Images\",\n",
    "    mask_dir=\"Mask_Data/Masks\",\n",
    "    transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "# Select only the first 10 instances\n",
    "limited_dataset = Subset(dataset, range(10))\n",
    "\n",
    "# Split into train (first 8) and validation (last 2)\n",
    "train_dataset = Subset(limited_dataset, range(8))\n",
    "val_dataset = Subset(limited_dataset, range(8, 10))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8  # Adjust batch size if needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "# Split dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda1=0.4, lambda2=0.7):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.bce = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "    def dice_loss(self, pred, target, smooth=1e-6):\n",
    "        intersection = (pred * target).sum(dim=(2, 3))\n",
    "        dice = (2. * intersection + smooth) / (pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) + smooth)\n",
    "        return 1 - dice.mean()  # Dice Loss (1 - Dice Coefficient)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Ensure target is a float tensor\n",
    "        target = target.float()  # Convert to float32\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        \n",
    "        # Apply sigmoid to pred for Dice loss\n",
    "        pred_sigmoid = torch.sigmoid(pred)\n",
    "        dice_loss = self.dice_loss(pred_sigmoid, target)\n",
    "        \n",
    "        return self.lambda2 * dice_loss + self.lambda1 * bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFFSUnet\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize model and optimizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:274\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m transformer_output\n\u001b[1;32m    273\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m ,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m448\u001b[39m, \u001b[38;5;241m448\u001b[39m)  \u001b[38;5;66;03m# Batch size=1, image=3, Channels=3, Height=448, Width=448\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFFS_UNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m model(input_tensor)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_tensor\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output: torch.Size([1, input_channels, 112, 112])\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:39\u001b[0m, in \u001b[0;36mFFS_UNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m=\u001b[39m Downsampling()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_plus2 \u001b[38;5;241m=\u001b[39m Downsampling()\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mttm \u001b[38;5;241m=\u001b[39m \u001b[43mTemporalTransformerModulePretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m704\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample \u001b[38;5;241m=\u001b[39m Upsampling([\u001b[38;5;241m704\u001b[39m, \u001b[38;5;241m352\u001b[39m, \u001b[38;5;241m176\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:196\u001b[0m, in \u001b[0;36mTemporalTransformerModulePretrained.__init__\u001b[0;34m(self, dim, num_heads, num_blocks, sr_ratio, output_channels)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Load pretrained ViT\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit \u001b[38;5;241m=\u001b[39m \u001b[43mViTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/vit-base-patch16-224-in21k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Adjust input/output projections\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m704\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/modeling_utils.py:3464\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3463\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3464\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3471\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3472\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3479\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:923\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1291\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import FFSUnet\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FFSUnet.FFS_UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "criterion = CombinedLoss()  # For binary masks\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "\n",
    "    for batch_idx, (input_stack, mask_t) in enumerate(train_loader):\n",
    "        input_stack = input_stack.to(device)  # Shape: (B, 3, C, H, W)\n",
    "        mask_t = mask_t.to(device)            # Shape: (B, 1, H, W)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_stack)\n",
    "        loss = criterion(outputs, mask_t)\n",
    "\n",
    "\n",
    "        # Check gradients for a sample batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_stack)\n",
    "        loss = criterion(outputs, mask_t)\n",
    "        loss.backward()\n",
    "\n",
    "        # Print gradients for key layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient for {name}: {param.grad.abs().mean().item()}\")\n",
    "            else:\n",
    "                print(f\"No gradient for {name}!\")\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Print batch loss\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.967552312360127e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.582944058290848e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.927064627663122e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.869950792797285e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.601016291051752e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5566336468517822e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.427090731828326e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.003928941674530506\n",
      "Gradient for fn.conv1.0.bias: 8.585867128374502e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.643396778192255e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.937004352365218e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.835499827094083e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0130078199888093e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5639509273138869e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4933761421486585e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266359696397558e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.740386192584992e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.984218991711196e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7505593329081304e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.594413821086164e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496048829139909e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.4800022524181526e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5598381418978347e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.5021695638686022e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.894206413157213e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.002603910630568862\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692370995879\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [1/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.906919563538395e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909353310766164e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5808849415248636e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.917334997367237e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.828367844774862e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.598442937195357e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5526342960880263e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4308251306861808e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.585648636483256e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.636149970871362e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.93853871527972e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.854727068420941e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0150012341031411e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080172725996817e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5611313240390667e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4907267586149187e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.526636151538696e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.748912705414114e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9885185038464046e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.746723967722963e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.599611486301059e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496048829139909e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.4760156410299234e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.1591739621508168e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.549694752947875e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4954326026193204e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.890068284515379e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352639714663383e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.00039771824958734214\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.0026039108633995056\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692370995879\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.02253352291882038\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [2/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.970394483303167e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.562879162365528e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.920685832601521e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.700131406323635e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.847140805391449e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.572150492411498e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5577690775769176e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.421388234976867e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.587032418461149e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.643679538118839e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.957593785301587e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.85613978385169e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0165605336676098e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5592198756089748e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4888342837228452e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266363334376365e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.652752819804377e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 3.0052323476970066e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.746723967722963e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.595524044110789e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496048829139909e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.48348416769509e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5555738392281776e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4965428256439456e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.890402489835048e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352639714663383e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765559691936\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.0026039108633995056\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692603826523\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [3/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.729817086015828e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.947183883194597e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5782204062657632e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.7689050057233544e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.923915670873355e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.822918102554864e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.556910404573955e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5559649651619017e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4224228348999612e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.585327115895325e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.642548932093371e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.946370991773755e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.890153374406905e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0108278146805927e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.557466314120251e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4791450399578854e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.526636151538696e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.697517012157263e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9910214929818046e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7518108274758304e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.588761875160997e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496048829139909e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.445534923253752e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5482943703168384e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4871563453356523e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.8909115227550297e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.00039771816227585077\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.002603910630568862\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.0028966928366571665\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578384935855865\n",
      "Epoch [4/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298172679147683e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.952394298627041e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5695405005132788e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.9197774880214125e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.841791893973491e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.578963618863396e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.552161177365008e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4298032701386132e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.583962873842665e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.6289842618729704e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.941283915180453e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.858158568296858e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0121196415691092e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5653261022444132e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4891434168672751e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266359696397558e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.680463986499021e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9823617533897284e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7534459127721558e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.564236137817202e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496048829139909e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.4517923960922523e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5490512789585048e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4923794893016012e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.8878266965237653e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352639714663383e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765559691936\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.0026039108633995056\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692370995879\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [5/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.729817086015828e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.935341272968799e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.573819846488079e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.922320809477629e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001316336973105e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.833616467491864e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.565842604171977e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5477454929670176e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4216153753320027e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.583526778238593e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.8941503337118775e-05\n",
      "Gradient for fn.stage2.0.bias: 4.63221388330437e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.943908551799606e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.854323203111691e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0147236783469848e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.561692669713871e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4869733862190454e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266359696397558e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.698464171175146e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.991606962154947e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7530018235623057e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.587046125223039e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496049966008286e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.485603782942299e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5564696612731937e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4928588421871114e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.894328385901618e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765559691936\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.0026039108633995056\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692603826523\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [6/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.901709148105951e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909353310766164e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.569116577463837e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.922664067885438e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.700131406323635e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.80475066885161e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.585952928168324e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5695273856627945e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4340107877194863e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.584255972721166e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.637018199971088e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.951497967007004e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.866434825580526e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0126445038408094e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5623361437032413e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.482273911902407e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266359696397558e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.779466043051798e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9892856853036553e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7464615907972214e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.589266571272289e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496049966008286e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.4763688198876106e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.546275721396942e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.489414602935668e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.893379980051236e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.002603910630568862\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.00289669306948781\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [7/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.947657115758844e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909353310766164e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5628388300447114e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.9082715008861273e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.811613668603444e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.585321922503938e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5650738897140387e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.4323894988958058e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.584843946835008e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149606116116e-05\n",
      "Gradient for fn.stage2.0.bias: 4.639278978341155e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.947259170193455e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.866636487184608e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.010388712800736e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5631814148219653e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4799146879750785e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.526636151538696e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.655357680575904e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9925556390558716e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7505996652289468e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.576851914296244e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496049966008286e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.461128460999525e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5498839462269737e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.488228756809512e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.892540536519189e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352639714663383e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765559691936\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.002603910630568862\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.0028966928366571665\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [8/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.947183883194597e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909357858239673e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5759394617352882e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.9252479385029053e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001316336973105e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.821807879530239e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996265574358404e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.58019960934003e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.5620333802465747e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.422555243090276e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.583953103880049e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149969913997e-05\n",
      "Gradient for fn.stage2.0.bias: 4.638148372315687e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.945846237922272e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.877940921136029e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0125840053595847e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080172725996817e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5571635506635845e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4782935346766696e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.5266359696397558e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.676673962648707e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9889222607354382e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7570792910925892e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.567667637693119e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496049966008286e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.4494207038399403e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5616674620133608e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4964292554663777e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.895856839914279e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.0026039108633995056\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.002896692370995879\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.022533521056175232\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [9/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Gradient for fn_minus2.conv1.0.weight: 1.7298169041168876e-05\n",
      "Gradient for fn_minus2.conv1.0.bias: 7.926814760139678e-11\n",
      "Gradient for fn_minus2.conv1.1.weight: 5.435498678707518e-06\n",
      "Gradient for fn_minus2.conv1.1.bias: 5.615085228782846e-06\n",
      "Gradient for fn_minus2.stage2.0.weight: 3.1909353310766164e-06\n",
      "Gradient for fn_minus2.stage2.0.bias: 2.5698635927606794e-12\n",
      "Gradient for fn_minus2.stage2.1.weight: 1.619170575395401e-06\n",
      "Gradient for fn_minus2.stage2.1.bias: 1.4580357401428046e-06\n",
      "Gradient for fn_minus2.stage2.3.weight: 2.768904778349679e-06\n",
      "Gradient for fn_minus2.stage2.3.bias: 1.9194747787748545e-12\n",
      "Gradient for fn_minus2.stage2.4.weight: 1.3480893130690674e-06\n",
      "Gradient for fn_minus2.stage2.4.bias: 1.4443720601775567e-06\n",
      "Gradient for fn_minus2.stage3.0.weight: 1.7001315200104727e-06\n",
      "Gradient for fn_minus2.stage3.0.bias: 5.805760061074194e-13\n",
      "Gradient for fn_minus2.stage3.1.weight: 9.466224923926347e-07\n",
      "Gradient for fn_minus2.stage3.1.bias: 1.000543420559552e-06\n",
      "Gradient for fn_minus2.stage3.3.weight: 1.5996264437490026e-06\n",
      "Gradient for fn_minus2.stage3.3.bias: 4.598342106393316e-13\n",
      "Gradient for fn_minus2.stage3.4.weight: 9.119463584283949e-07\n",
      "Gradient for fn_minus2.stage3.4.bias: 9.930162150340038e-07\n",
      "Gradient for fn_minus2.stage4.0.weight: 1.1191679050170933e-06\n",
      "Gradient for fn_minus2.stage4.0.bias: 1.562664114860418e-13\n",
      "Gradient for fn_minus2.stage4.1.weight: 7.372001959993213e-07\n",
      "Gradient for fn_minus2.stage4.1.bias: 7.505774419769295e-07\n",
      "Gradient for fn_minus2.stage4.3.weight: 1.1914105471078074e-06\n",
      "Gradient for fn_minus2.stage4.3.bias: 1.432061527738629e-13\n",
      "Gradient for fn_minus2.stage4.4.weight: 8.282198677989072e-07\n",
      "Gradient for fn_minus2.stage4.4.bias: 8.493564109812723e-07\n",
      "Gradient for fn.conv1.0.weight: 0.0039289421401917934\n",
      "Gradient for fn.conv1.0.bias: 8.582787813793402e-09\n",
      "Gradient for fn.conv1.1.weight: 0.0019034909782931209\n",
      "Gradient for fn.conv1.1.bias: 0.002130233682692051\n",
      "Gradient for fn.stage2.0.weight: 4.894149606116116e-05\n",
      "Gradient for fn.stage2.0.bias: 4.645697888883138e-12\n",
      "Gradient for fn.stage2.1.weight: 2.62458579527447e-05\n",
      "Gradient for fn.stage2.1.bias: 2.9449012799886987e-05\n",
      "Gradient for fn.stage2.3.weight: 3.660799848148599e-05\n",
      "Gradient for fn.stage2.3.bias: 7.941688105750355e-12\n",
      "Gradient for fn.stage2.4.weight: 4.108845314476639e-05\n",
      "Gradient for fn.stage2.4.bias: 6.187882536323741e-05\n",
      "Gradient for fn.stage3.0.weight: 4.713852831628174e-06\n",
      "Gradient for fn.stage3.0.bias: 7.877940921136029e-13\n",
      "Gradient for fn.stage3.1.weight: 3.148514451822848e-06\n",
      "Gradient for fn.stage3.1.bias: 3.6171179544908227e-06\n",
      "Gradient for fn.stage3.3.weight: 5.0751650633173995e-06\n",
      "Gradient for fn.stage3.3.bias: 1.0153392883405221e-12\n",
      "Gradient for fn.stage3.4.weight: 7.134268344088923e-06\n",
      "Gradient for fn.stage3.4.bias: 1.0622658919601236e-05\n",
      "Gradient for fn.stage4.0.weight: 1.1080173862865195e-06\n",
      "Gradient for fn.stage4.0.bias: 1.5693760039344612e-13\n",
      "Gradient for fn.stage4.1.weight: 7.869482487876667e-07\n",
      "Gradient for fn.stage4.1.bias: 8.00895975316962e-07\n",
      "Gradient for fn.stage4.3.weight: 1.2740663350996329e-06\n",
      "Gradient for fn.stage4.3.bias: 1.4899950576737625e-13\n",
      "Gradient for fn.stage4.4.weight: 8.279634471364261e-07\n",
      "Gradient for fn.stage4.4.bias: 8.561990512134798e-07\n",
      "Gradient for fn_plus2.conv1.0.weight: 1.526636151538696e-05\n",
      "Gradient for fn_plus2.conv1.0.bias: 8.638541965089175e-11\n",
      "Gradient for fn_plus2.conv1.1.weight: 6.36303229839541e-06\n",
      "Gradient for fn_plus2.conv1.1.bias: 5.248994511930505e-06\n",
      "Gradient for fn_plus2.stage2.0.weight: 3.397787622816395e-06\n",
      "Gradient for fn_plus2.stage2.0.bias: 2.9901738637233555e-12\n",
      "Gradient for fn_plus2.stage2.1.weight: 1.6345228459613281e-06\n",
      "Gradient for fn_plus2.stage2.1.bias: 1.4688877172375214e-06\n",
      "Gradient for fn_plus2.stage2.3.weight: 2.431606162645039e-06\n",
      "Gradient for fn_plus2.stage2.3.bias: 1.7487829760687301e-12\n",
      "Gradient for fn_plus2.stage2.4.weight: 1.5641165873603313e-06\n",
      "Gradient for fn_plus2.stage2.4.bias: 1.4304608839665889e-06\n",
      "Gradient for fn_plus2.stage3.0.weight: 1.682976972006145e-06\n",
      "Gradient for fn_plus2.stage3.0.bias: 5.571250383772097e-13\n",
      "Gradient for fn_plus2.stage3.1.weight: 1.0368169114371995e-06\n",
      "Gradient for fn_plus2.stage3.1.bias: 1.104193074752402e-06\n",
      "Gradient for fn_plus2.stage3.3.weight: 1.6496049966008286e-06\n",
      "Gradient for fn_plus2.stage3.3.bias: 4.466124464610338e-13\n",
      "Gradient for fn_plus2.stage3.4.weight: 1.0336821105738636e-06\n",
      "Gradient for fn_plus2.stage3.4.bias: 1.053858454724832e-06\n",
      "Gradient for fn_plus2.stage4.0.weight: 1.159173848463979e-06\n",
      "Gradient for fn_plus2.stage4.0.bias: 1.5484331481949165e-13\n",
      "Gradient for fn_plus2.stage4.1.weight: 7.617309165652841e-07\n",
      "Gradient for fn_plus2.stage4.1.bias: 7.834150324015354e-07\n",
      "Gradient for fn_plus2.stage4.3.weight: 1.2202234529468114e-06\n",
      "Gradient for fn_plus2.stage4.3.bias: 1.4975016669402375e-13\n",
      "Gradient for fn_plus2.stage4.4.weight: 8.096416763692105e-07\n",
      "Gradient for fn_plus2.stage4.4.bias: 7.953993303999596e-07\n",
      "Gradient for bottleneck.0.weight: 1.331032649432018e-06\n",
      "Gradient for bottleneck.0.bias: 2.8883671713067494e-13\n",
      "Gradient for bottleneck.1.weight: 2.411976538496674e-06\n",
      "Gradient for bottleneck.1.bias: 3.403779373911675e-06\n",
      "Gradient for bottleneck.3.weight: 7.37219670554623e-06\n",
      "Gradient for bottleneck.3.bias: 2.0065395801793784e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.weight: 1.2352640624158084e-05\n",
      "Gradient for upsample.upsampling_stages.0.conv.bias: 5.318958210409619e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.weight: 5.4797881603008136e-05\n",
      "Gradient for upsample.upsampling_stages.1.conv.bias: 0.0002960765268653631\n",
      "Gradient for upsample.upsampling_stages.2.conv.weight: 0.0003977182204835117\n",
      "Gradient for upsample.upsampling_stages.2.conv.bias: 0.002603910630568862\n",
      "Gradient for upsample.upsampling_stages.3.conv.weight: 0.0028966928366571665\n",
      "Gradient for upsample.upsampling_stages.3.conv.bias: 0.012348459102213383\n",
      "Gradient for upsample.conv11.weight: 0.02253352291882038\n",
      "Gradient for upsample.conv11.bias: 0.21578386425971985\n",
      "Epoch [10/50], Loss: 1.0074\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stack\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, mask_t)\n\u001b[1;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:31\u001b[0m, in \u001b[0;36mSimplifiedFFS_UNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m concatenated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fn_minus2, fn, fn_plus2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(concatenated_features)\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:149\u001b[0m, in \u001b[0;36mUpsampling.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stage, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampling_stages, y):\n\u001b[0;32m--> 149\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_upsample(x)\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv11(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:126\u001b[0m, in \u001b[0;36mUpsampling_Stage.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(x)\n\u001b[1;32m    125\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcatenated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import FFSUnet\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FFSUnet.SimplifiedFFS_UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "criterion = CombinedLoss()  # For binary masks\n",
    "\n",
    "# Get a single sample\n",
    "#single_sample = dataset\n",
    "#train_loader = DataLoader([single_sample], batch_size=8, shuffle=True)\n",
    "\n",
    "# Track a specific weight (e.g., first convolutional layer)\n",
    "def print_weights(model, epoch, prefix=\"\"):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{prefix}Epoch {epoch+1} - {name}:\")\n",
    "        print(f\"  Weight mean: {param.data.mean().item():.6f}\")\n",
    "        if param.grad is not None:\n",
    "            print(f\"  Gradient mean: {param.grad.mean().item():.6f}\")\n",
    "        else:\n",
    "            print(\"  Gradient: None\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "\n",
    "    for batch_idx, (input_stack, mask_t) in enumerate(train_loader):\n",
    "        input_stack = input_stack.to(device)  # Shape: (B, 3, C, H, W)\n",
    "        mask_t = mask_t.to(device)            # Shape: (B, 1, H, W)\n",
    "\n",
    "        # Check gradients for a sample batch\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_stack)\n",
    "        loss = criterion(outputs, mask_t)\n",
    "        loss.backward()\n",
    "\n",
    "        # Print gradients for key layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Gradient for {name}: {param.grad.abs().mean().item()}\")\n",
    "            else:\n",
    "                print(f\"No gradient for {name}!\")\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Print batch loss\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Unique values in mask_t: tensor([0, 1], dtype=torch.uint8)\n",
      "Epoch 1/50 - Loss: 0.9394\n",
      "Epoch 2/50 - Loss: 0.8875\n",
      "Epoch 3/50 - Loss: 0.7402\n",
      "Epoch 4/50 - Loss: 0.7103\n",
      "Epoch 5/50 - Loss: 0.7015\n",
      "Epoch 6/50 - Loss: 0.6353\n",
      "Epoch 7/50 - Loss: 0.5807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass using the single sample\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, mask_sample)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:31\u001b[0m, in \u001b[0;36mSimplifiedFFS_UNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m concatenated_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([fn_minus2, fn, fn_plus2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(concatenated_features)\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:149\u001b[0m, in \u001b[0;36mUpsampling.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stage, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampling_stages, y):\n\u001b[0;32m--> 149\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_upsample(x)\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv11(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/attentionCNN/FFS-UNet/FFSUnet.py:127\u001b[0m, in \u001b[0;36mUpsampling_Stage.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    125\u001b[0m concatenated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(concatenated)\n\u001b[0;32m--> 127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import FFSUnet\n",
    "\n",
    "# Assume CombinedLoss is defined/imported somewhere\n",
    "# from your_loss_module import CombinedLoss\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FFSUnet.FFS_UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004)\n",
    "criterion = CombinedLoss()  # For binary masks\n",
    "\n",
    "# Grab a single sample from the training loader\n",
    "# This assumes that train_loader returns a tuple: (input_stack, mask_t)\n",
    "single_sample = next(iter(train_loader))\n",
    "input_sample, mask_sample = single_sample\n",
    "input_sample = input_sample.to(device)  # Shape: (B, 3, C, H, W)\n",
    "mask_sample = mask_sample.to(device)    # Shape: (B, 1, H, W)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass using the single sample\n",
    "    outputs = model(input_sample)\n",
    "    loss = criterion(outputs, mask_sample)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print gradients for key layers (optional)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
